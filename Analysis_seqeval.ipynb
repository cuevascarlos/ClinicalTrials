{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis seqeval classification_report output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import sequence_labeling\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `y_pred` and `y_true` has been obtained analysing where the classification_report varies its output from `model=default` mode to `mode=strict` when adding iteratively the predictions and true_labels of the age entity. \n",
    "\n",
    "**REMARK:** There was a file where a `B-age` was labeled as `I-elegibility`, this sample has been omitted to simplify the analysis.\n",
    "\n",
    "What each mode options makes?\n",
    "mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n",
    "        If you want to only count exact matches, pass mode=\"strict\". default: None.\n",
    "\n",
    "- `Default/Lenient`: Consider as correct if the entity was correctly identified even the prefix B or I is bad.\n",
    "- `Strict`: To consider a prediction as correct both, the entity and the prefix, have to be correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       1.00      0.77      0.87        13\n",
      "\n",
      "   micro avg       1.00      0.77      0.87        13\n",
      "   macro avg       1.00      0.77      0.87        13\n",
      "weighted avg       1.00      0.77      0.87        13\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       1.00      0.77      0.87        13\n",
      "\n",
      "   micro avg       1.00      0.77      0.87        13\n",
      "   macro avg       1.00      0.77      0.87        13\n",
      "weighted avg       1.00      0.77      0.87        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lists where both modes of classification_report give the same results\n",
    "y_pred_1 = [['O', 'O', 'O'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age']]\n",
    "y_true_1 = [['B-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age']]\n",
    "\n",
    "#Show the results\n",
    "print(classification_report(y_true_1, y_pred_1))\n",
    "print(classification_report(y_true_1, y_pred_1, mode='strict', scheme=IOB2))\n",
    "\n",
    "report_lenient_1 = classification_report(y_true_1, y_pred_1, output_dict=True)\n",
    "report_strict_1 = classification_report(y_true_1, y_pred_1, mode='strict', scheme=IOB2, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, during the evaluation process the following file that contains `age` tag predicted the tokens `[['age', '(', 'y', ')', '51', '.', '9', 'Â±', '8', '.', '8']]` as: `[['O', 'O', 'I-age', 'O', 'B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age']]`.\n",
    "Obviously, the true label is `[['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age']]`.\n",
    "\n",
    "If we add, this new prediction to the prediction and true lists we obtain the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       0.83      0.71      0.77        14\n",
      "\n",
      "   micro avg       0.83      0.71      0.77        14\n",
      "   macro avg       0.83      0.71      0.77        14\n",
      "weighted avg       0.83      0.71      0.77        14\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       0.91      0.71      0.80        14\n",
      "\n",
      "   micro avg       0.91      0.71      0.80        14\n",
      "   macro avg       0.91      0.71      0.80        14\n",
      "weighted avg       0.91      0.71      0.80        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "added_pred = [['O', 'O', 'I-age', 'O', 'B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age']]\n",
    "added_true = [['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age']]\n",
    "\n",
    "y_pred_2 = y_pred_1 + added_pred\n",
    "y_true_2 = y_true_1 + added_true\n",
    "\n",
    "#Show the results\n",
    "print(classification_report(y_true_2, y_pred_2))\n",
    "print(classification_report(y_true_2, y_pred_2, mode='strict', scheme=IOB2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this new prediction change the output of the reports. But, why strict mode has higher values if it is an approach more restrictive? To analyse that we are going to compute how the internally each report compute the True Positive, False Negative, True Negative and False Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_results(y_true, y_pred, TP,FN, FP, mode=None):\n",
    "    #Proof that computing precision, recall and F1-score from the confusion matrix gives the same results as the seqeval library if not assert an error\n",
    "    if TP == 0:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "    else:\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "    report = classification_report(y_true, y_pred, mode=mode, scheme=IOB2, output_dict=True)\n",
    "\n",
    "    #Assert an error if the results are different otherwise print that the results are the same\n",
    "    assert report['age']['precision'] == precision\n",
    "    assert report['age']['recall'] == recall\n",
    "    assert report['age']['f1-score'] == f1\n",
    "    print('Results are the same with cm and seqeval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cm(y_true, y_pred, suffix = False):\n",
    "    '''\n",
    "    Compute the confusion matrix for a sequence labeling task\n",
    "    Args:\n",
    "        y_true: list of lists of strings, the true labels\n",
    "        y_pred: list of lists of strings, the predicted labels\n",
    "        suffix: boolean, whether the entities are prefixed with the entity type\n",
    "                False: Consider B-entity and I-entity as same entity\n",
    "                True: Consider B-entity and I-entity as different entities\n",
    "    Returns:\n",
    "        TP, FN, FP: integers, the number of True Positives, False Negatives, False Positives\n",
    "    '''\n",
    "    if suffix == True:\n",
    "        mode = 'strict'    \n",
    "        aux_pred_entities = [sequence_labeling.get_entities(example, suffix=False) for example in y_pred]\n",
    "        for entity in aux_pred_entities:\n",
    "            for subentity in entity:\n",
    "                if subentity[0] != 'age':\n",
    "                    #It will be considered as FN\n",
    "                    print(entity)\n",
    "                    entity.remove(subentity)\n",
    "                    print(entity)\n",
    "    else:\n",
    "        mode = None\n",
    "\n",
    "    print(f\"Mode: {mode}\")\n",
    "\n",
    "    #Get the entities\n",
    "    true_entities = [sequence_labeling.get_entities(example, suffix=suffix) for example in y_true]\n",
    "    pred_entities = [sequence_labeling.get_entities(example, suffix=suffix) for example in y_pred]\n",
    "\n",
    "    #Count the TP, FN, FP and TN\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "\n",
    "    for true, pred in zip(true_entities, pred_entities):\n",
    "        print(f\"True: {true} - Predicted: {pred}\")\n",
    "        for entity in true:\n",
    "            if entity in pred:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        for entity in pred:\n",
    "            if entity not in true:\n",
    "                if mode == 'strict':      \n",
    "                    if entity[0] == 'B':  \n",
    "                        FP += 1*2  #Everything is computed twice in strict mode due to the split in B- and I-\n",
    "                else:\n",
    "                    if entity[0] == 'age': #If the entity is not in the true labels and the entity is age, otherwise it is false negative\n",
    "                        FP += 1\n",
    "                    \n",
    "    print(f\"True positives: {TP} - False negatives: {FN} - False positives: {FP}\")\n",
    "\n",
    "    #Test if the results are the same as the seqeval library\n",
    "    test_results(y_true, y_pred, TP, FN, FP, mode=mode)\n",
    "\n",
    "    return TP, FN, FP, true_entities, pred_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the first test\n",
      "Mode: None\n",
      "True: [('age', 0, 2)] - Predicted: []\n",
      "True: [('age', 0, 5)] - Predicted: [('age', 0, 5)]\n",
      "True: [('age', 0, 7)] - Predicted: [('age', 0, 7)]\n",
      "True: [('age', 0, 2)] - Predicted: []\n",
      "True: [('age', 0, 4)] - Predicted: []\n",
      "True: [('age', 0, 5)] - Predicted: [('age', 0, 5)]\n",
      "True: [('age', 0, 2)] - Predicted: [('age', 0, 2)]\n",
      "True: [('age', 0, 3)] - Predicted: [('age', 0, 3)]\n",
      "True: [('age', 0, 8)] - Predicted: [('age', 0, 8)]\n",
      "True: [('age', 0, 7)] - Predicted: [('age', 0, 7)]\n",
      "True: [('age', 0, 6)] - Predicted: [('age', 0, 6)]\n",
      "True: [('age', 0, 2)] - Predicted: [('age', 0, 2)]\n",
      "True: [('age', 0, 2)] - Predicted: [('age', 0, 2)]\n",
      "True positives: 10 - False negatives: 3 - False positives: 0\n",
      "Results are the same with cm and seqeval\n",
      "Mode: strict\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: []\n",
      "True: [('B', 0, 0), ('I', 1, 5)] - Predicted: [('B', 0, 0), ('I', 1, 5)]\n",
      "True: [('B', 0, 0), ('I', 1, 7)] - Predicted: [('B', 0, 0), ('I', 1, 7)]\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: []\n",
      "True: [('B', 0, 0), ('I', 1, 4)] - Predicted: []\n",
      "True: [('B', 0, 0), ('I', 1, 5)] - Predicted: [('B', 0, 0), ('I', 1, 5)]\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: [('B', 0, 0), ('I', 1, 2)]\n",
      "True: [('B', 0, 0), ('I', 1, 3)] - Predicted: [('B', 0, 0), ('I', 1, 3)]\n",
      "True: [('B', 0, 0), ('I', 1, 8)] - Predicted: [('B', 0, 0), ('I', 1, 8)]\n",
      "True: [('B', 0, 0), ('I', 1, 7)] - Predicted: [('B', 0, 0), ('I', 1, 7)]\n",
      "True: [('B', 0, 0), ('I', 1, 6)] - Predicted: [('B', 0, 0), ('I', 1, 6)]\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: [('B', 0, 0), ('I', 1, 2)]\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: [('B', 0, 0), ('I', 1, 2)]\n",
      "True positives: 20 - False negatives: 6 - False positives: 0\n",
      "Results are the same with cm and seqeval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/MASTER/ClinicalTrials/venv/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: B-age seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/carlos/MASTER/ClinicalTrials/venv/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: I-age seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "print('Results for the first test')\n",
    "TP_1_len, FN_1_len, FP_1_len, true_entities_1_len, pred_entities_1_len = compute_cm(y_true_1, y_pred_1)\n",
    "TP_1_str, FN_1_str, FP_1_str, true_entities_1_str, pred_entities_1_str = compute_cm(y_true_1, y_pred_1, suffix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the second test\n",
      "Mode: None\n",
      "True: [('age', 0, 2)] - Predicted: []\n",
      "True: [('age', 0, 5)] - Predicted: [('age', 0, 5)]\n",
      "True: [('age', 0, 7)] - Predicted: [('age', 0, 7)]\n",
      "True: [('age', 0, 2)] - Predicted: []\n",
      "True: [('age', 0, 4)] - Predicted: []\n",
      "True: [('age', 0, 5)] - Predicted: [('age', 0, 5)]\n",
      "True: [('age', 0, 2)] - Predicted: [('age', 0, 2)]\n",
      "True: [('age', 0, 3)] - Predicted: [('age', 0, 3)]\n",
      "True: [('age', 0, 8)] - Predicted: [('age', 0, 8)]\n",
      "True: [('age', 0, 7)] - Predicted: [('age', 0, 7)]\n",
      "True: [('age', 0, 6)] - Predicted: [('age', 0, 6)]\n",
      "True: [('age', 0, 2)] - Predicted: [('age', 0, 2)]\n",
      "True: [('age', 0, 2)] - Predicted: [('age', 0, 2)]\n",
      "True: [('age', 0, 10)] - Predicted: [('age', 2, 2), ('age', 4, 10)]\n",
      "True positives: 10 - False negatives: 4 - False positives: 2\n",
      "Results are the same with cm and seqeval\n",
      "Mode: strict\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: []\n",
      "True: [('B', 0, 0), ('I', 1, 5)] - Predicted: [('B', 0, 0), ('I', 1, 5)]\n",
      "True: [('B', 0, 0), ('I', 1, 7)] - Predicted: [('B', 0, 0), ('I', 1, 7)]\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: []\n",
      "True: [('B', 0, 0), ('I', 1, 4)] - Predicted: []\n",
      "True: [('B', 0, 0), ('I', 1, 5)] - Predicted: [('B', 0, 0), ('I', 1, 5)]\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: [('B', 0, 0), ('I', 1, 2)]\n",
      "True: [('B', 0, 0), ('I', 1, 3)] - Predicted: [('B', 0, 0), ('I', 1, 3)]\n",
      "True: [('B', 0, 0), ('I', 1, 8)] - Predicted: [('B', 0, 0), ('I', 1, 8)]\n",
      "True: [('B', 0, 0), ('I', 1, 7)] - Predicted: [('B', 0, 0), ('I', 1, 7)]\n",
      "True: [('B', 0, 0), ('I', 1, 6)] - Predicted: [('B', 0, 0), ('I', 1, 6)]\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: [('B', 0, 0), ('I', 1, 2)]\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: [('B', 0, 0), ('I', 1, 2)]\n",
      "True: [('B', 0, 0), ('I', 1, 10)] - Predicted: [('I', 2, 2), ('B', 4, 4), ('I', 5, 10)]\n",
      "True positives: 20 - False negatives: 8 - False positives: 2\n",
      "Results are the same with cm and seqeval\n"
     ]
    }
   ],
   "source": [
    "print('Results for the second test')\n",
    "TP_2_len, FN_2_len, FP_2_len, true_entities_2_len, pred_entities_2_len = compute_cm(y_true_2, y_pred_2)\n",
    "TP_2_str, FN_2_str, FP_2_str, true_entities_2_str, pred_entities_2_str = compute_cm(y_true_2, y_pred_2, suffix=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have obtained a way to compute manually the outputs of the report in both modes. Now, how to identify the reason of the discrepancy? Let see the case considering only the last predictions added, where the problem starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the added predictions\n",
      "Mode: None\n",
      "True: [('age', 0, 10)] - Predicted: [('age', 2, 2), ('age', 4, 10)]\n",
      "True positives: 0 - False negatives: 1 - False positives: 2\n",
      "Results are the same with cm and seqeval\n",
      "Mode: strict\n",
      "True: [('B', 0, 0), ('I', 1, 10)] - Predicted: [('I', 2, 2), ('B', 4, 4), ('I', 5, 10)]\n",
      "True positives: 0 - False negatives: 2 - False positives: 2\n",
      "Results are the same with cm and seqeval\n"
     ]
    }
   ],
   "source": [
    "print('Results for the added predictions')\n",
    "TP_3_len, FN_3_len, FP_3_len, true_entities_3_len, pred_entities_3_len = compute_cm(added_true, added_pred)\n",
    "TP_3_str, FN_3_str, FP_3_str, true_entities_3_str, pred_entities_3_str = compute_cm(added_true, added_pred, suffix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the number of False negatives and False positives changes in each of the cases. We add then to the obtained for the first test and see that the reported values are in each case the ones for the second test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TP_1_len + TP_3_len == TP_2_len\n",
    "assert FN_1_len + FN_3_len == FN_2_len\n",
    "assert FP_1_len + FP_3_len == FP_2_len\n",
    "assert TP_1_str + TP_3_str == TP_2_str\n",
    "assert FN_1_str + FN_3_str == FN_2_str\n",
    "assert FP_1_str + FP_3_str == FP_2_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the problem in the difference is caused by the number of TP, FN, FP. In each of the modes it is computed in different ways and it can cause the precision or recall in strict mode to be higher than in default mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The case of Elegibility entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scenario will be used to verify the conclusions with a more difficult scenario where a new entity is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = [['O', 'O', 'O'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age']]\n",
    "y_true_1 = [['B-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age'], ['B-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age', 'I-age']]\n",
    "\n",
    "added_pred = [['I-eligibility', 'I-age', 'I-age', 'I-age']]\n",
    "added_true = [['B-age', 'I-age', 'I-age', 'I-age']]\n",
    "\n",
    "y_pred_2 = y_pred_1 + added_pred\n",
    "y_true_2 = y_true_1 + added_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the first test\n",
      "Mode: None\n",
      "True: [('age', 0, 2)] - Predicted: []\n",
      "True: [('age', 0, 5)] - Predicted: [('age', 0, 5)]\n",
      "True: [('age', 0, 7)] - Predicted: [('age', 0, 7)]\n",
      "True: [('age', 0, 2)] - Predicted: []\n",
      "True: [('age', 0, 4)] - Predicted: []\n",
      "True: [('age', 0, 5)] - Predicted: [('age', 0, 5)]\n",
      "True: [('age', 0, 2)] - Predicted: [('age', 0, 2)]\n",
      "True: [('age', 0, 3)] - Predicted: [('age', 0, 3)]\n",
      "True: [('age', 0, 8)] - Predicted: [('age', 0, 8)]\n",
      "True: [('age', 0, 7)] - Predicted: [('age', 0, 7)]\n",
      "True positives: 7 - False negatives: 3 - False positives: 0\n",
      "Results are the same with cm and seqeval\n",
      "Mode: strict\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: []\n",
      "True: [('B', 0, 0), ('I', 1, 5)] - Predicted: [('B', 0, 0), ('I', 1, 5)]\n",
      "True: [('B', 0, 0), ('I', 1, 7)] - Predicted: [('B', 0, 0), ('I', 1, 7)]\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: []\n",
      "True: [('B', 0, 0), ('I', 1, 4)] - Predicted: []\n",
      "True: [('B', 0, 0), ('I', 1, 5)] - Predicted: [('B', 0, 0), ('I', 1, 5)]\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: [('B', 0, 0), ('I', 1, 2)]\n",
      "True: [('B', 0, 0), ('I', 1, 3)] - Predicted: [('B', 0, 0), ('I', 1, 3)]\n",
      "True: [('B', 0, 0), ('I', 1, 8)] - Predicted: [('B', 0, 0), ('I', 1, 8)]\n",
      "True: [('B', 0, 0), ('I', 1, 7)] - Predicted: [('B', 0, 0), ('I', 1, 7)]\n",
      "True positives: 14 - False negatives: 6 - False positives: 0\n",
      "Results are the same with cm and seqeval\n"
     ]
    }
   ],
   "source": [
    "print('Results for the first test')\n",
    "TP_1_len, FN_1_len, FP_1_len, true_entities_1_len, pred_entities_1_len = compute_cm(y_true_1, y_pred_1)\n",
    "TP_1_str, FN_1_str, FP_1_str, true_entities_1_str, pred_entities_1_str = compute_cm(y_true_1, y_pred_1, suffix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the second test\n",
      "Mode: None\n",
      "True: [('age', 0, 2)] - Predicted: []\n",
      "True: [('age', 0, 5)] - Predicted: [('age', 0, 5)]\n",
      "True: [('age', 0, 7)] - Predicted: [('age', 0, 7)]\n",
      "True: [('age', 0, 2)] - Predicted: []\n",
      "True: [('age', 0, 4)] - Predicted: []\n",
      "True: [('age', 0, 5)] - Predicted: [('age', 0, 5)]\n",
      "True: [('age', 0, 2)] - Predicted: [('age', 0, 2)]\n",
      "True: [('age', 0, 3)] - Predicted: [('age', 0, 3)]\n",
      "True: [('age', 0, 8)] - Predicted: [('age', 0, 8)]\n",
      "True: [('age', 0, 7)] - Predicted: [('age', 0, 7)]\n",
      "True: [('age', 0, 3)] - Predicted: [('eligibility', 0, 0), ('age', 1, 3)]\n",
      "True positives: 7 - False negatives: 4 - False positives: 1\n",
      "Results are the same with cm and seqeval\n",
      "[('eligibility', 0, 0), ('age', 1, 3)]\n",
      "[('age', 1, 3)]\n",
      "Mode: strict\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: []\n",
      "True: [('B', 0, 0), ('I', 1, 5)] - Predicted: [('B', 0, 0), ('I', 1, 5)]\n",
      "True: [('B', 0, 0), ('I', 1, 7)] - Predicted: [('B', 0, 0), ('I', 1, 7)]\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: []\n",
      "True: [('B', 0, 0), ('I', 1, 4)] - Predicted: []\n",
      "True: [('B', 0, 0), ('I', 1, 5)] - Predicted: [('B', 0, 0), ('I', 1, 5)]\n",
      "True: [('B', 0, 0), ('I', 1, 2)] - Predicted: [('B', 0, 0), ('I', 1, 2)]\n",
      "True: [('B', 0, 0), ('I', 1, 3)] - Predicted: [('B', 0, 0), ('I', 1, 3)]\n",
      "True: [('B', 0, 0), ('I', 1, 8)] - Predicted: [('B', 0, 0), ('I', 1, 8)]\n",
      "True: [('B', 0, 0), ('I', 1, 7)] - Predicted: [('B', 0, 0), ('I', 1, 7)]\n",
      "True: [('B', 0, 0), ('I', 1, 3)] - Predicted: [('I', 0, 3)]\n",
      "True positives: 14 - False negatives: 8 - False positives: 0\n",
      "Results are the same with cm and seqeval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/MASTER/ClinicalTrials/venv/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/carlos/MASTER/ClinicalTrials/venv/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: I-eligibility seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "print('Results for the second test')\n",
    "TP_2_len, FN_2_len, FP_2_len, true_entities_2_len, pred_entities_2_len = compute_cm(y_true_2, y_pred_2)\n",
    "TP_2_str, FN_2_str, FP_2_str, true_entities_2_str, pred_entities_2_str = compute_cm(y_true_2, y_pred_2, suffix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the added predictions\n",
      "Mode: None\n",
      "True: [('age', 0, 3)] - Predicted: [('eligibility', 0, 0), ('age', 1, 3)]\n",
      "True positives: 0 - False negatives: 1 - False positives: 1\n",
      "Results are the same with cm and seqeval\n",
      "[('eligibility', 0, 0), ('age', 1, 3)]\n",
      "[('age', 1, 3)]\n",
      "Mode: strict\n",
      "True: [('B', 0, 0), ('I', 1, 3)] - Predicted: [('I', 0, 3)]\n",
      "True positives: 0 - False negatives: 2 - False positives: 0\n",
      "Results are the same with cm and seqeval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/MASTER/ClinicalTrials/venv/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Results for the added predictions')\n",
    "TP_3_len, FN_3_len, FP_3_len, true_entities_3_len, pred_entities_3_len = compute_cm(added_true, added_pred)\n",
    "TP_3_str, FN_3_str, FP_3_str, true_entities_3_str, pred_entities_3_str = compute_cm(added_true, added_pred, suffix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TP_1_len + TP_3_len == TP_2_len\n",
    "assert FN_1_len + FN_3_len == FN_2_len\n",
    "assert FP_1_len + FP_3_len == FP_2_len\n",
    "assert TP_1_str + TP_3_str == TP_2_str\n",
    "assert FN_1_str + FN_3_str == FN_2_str\n",
    "assert FP_1_str + FP_3_str == FP_2_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the difference in the support once eligibility appears as happended before. Let see how it affect to the reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       0.88      0.64      0.74        11\n",
      " eligibility       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.78      0.64      0.70        11\n",
      "   macro avg       0.44      0.32      0.37        11\n",
      "weighted avg       0.88      0.64      0.74        11\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       1.00      0.64      0.78        11\n",
      "\n",
      "   micro avg       1.00      0.64      0.78        11\n",
      "   macro avg       1.00      0.64      0.78        11\n",
      "weighted avg       1.00      0.64      0.78        11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_2, y_pred_2)) \n",
    "print(classification_report(y_true_2, y_pred_2, mode='strict', scheme=IOB2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more time strict mode reports higher values due to the difference in TP, FN and FP. Additionally, it can be highlighted that eligibility appears in default mode with support 0 because it does not have B-eligibility in the predicted labels but it appears. Additionally, it was not count in strict mode because it does not appear in true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easier examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Default mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "Mode: None\n",
      "True: [('age', 0, 1)] - Predicted: [('age', 0, 1)]\n",
      "True positives: 1 - False negatives: 0 - False positives: 0\n",
      "Results are the same with cm and seqeval\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-age', 'I-age']]\n",
    "y_pred = [['I-age', 'I-age']]\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "TP_1_len, FN_1_len, FP_1_len, true_entities_1_len, pred_entities_1_len = compute_cm(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "Mode: None\n",
      "True: [('age', 0, 1)] - Predicted: [('age', 0, 1)]\n",
      "True positives: 1 - False negatives: 0 - False positives: 0\n",
      "Results are the same with cm and seqeval\n"
     ]
    }
   ],
   "source": [
    "y_true = [['I-age', 'I-age']]\n",
    "y_pred = [['B-age', 'I-age']]\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "TP_1_len, FN_1_len, FP_1_len, true_entities_1_len, pred_entities_1_len = compute_cm(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Strict mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.00      0.00      0.00         1\n",
      "   macro avg       0.00      0.00      0.00         1\n",
      "weighted avg       0.00      0.00      0.00         1\n",
      "\n",
      "Mode: strict\n",
      "True: [('B', 0, 0), ('I', 1, 1)] - Predicted: [('I', 0, 1)]\n",
      "True positives: 0 - False negatives: 2 - False positives: 0\n",
      "Results are the same with cm and seqeval\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-age', 'I-age']]\n",
    "y_pred = [['I-age', 'I-age']]\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode='strict', scheme=IOB2))\n",
    "TP_1_len, FN_1_len, FP_1_len, true_entities_1_len, pred_entities_1_len = compute_cm(y_true, y_pred, suffix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.00      0.00      0.00         0\n",
      "   macro avg       0.00      0.00      0.00         0\n",
      "weighted avg       0.00      0.00      0.00         0\n",
      "\n",
      "Mode: strict\n",
      "True: [('I', 0, 1)] - Predicted: [('B', 0, 0), ('I', 1, 1)]\n",
      "True positives: 0 - False negatives: 1 - False positives: 2\n",
      "Results are the same with cm and seqeval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/MASTER/ClinicalTrials/venv/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_true = [['I-age', 'I-age']]\n",
    "y_pred = [['B-age', 'I-age']]\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode='strict', scheme=IOB2))\n",
    "TP_1_len, FN_1_len, FP_1_len, true_entities_1_len, pred_entities_1_len = compute_cm(y_true, y_pred, suffix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m y_true \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI-age\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI-age\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI-age\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI-age\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheme\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIOB2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m TP_1_len, FN_1_len, FP_1_len, true_entities_1_len, pred_entities_1_len \u001b[38;5;241m=\u001b[39m compute_cm(y_true, y_pred, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/MASTER/ClinicalTrials/venv/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:670\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, digits, suffix, output_dict, mode, sample_weight, zero_division, scheme)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcr\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdigits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdigits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m              \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m              \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m              \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m              \u001b[49m\u001b[43mscheme\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m              \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m              \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m target_names_true \u001b[38;5;241m=\u001b[39m {type_name \u001b[38;5;28;01mfor\u001b[39;00m type_name, _, _ \u001b[38;5;129;01min\u001b[39;00m get_entities(y_true, suffix)}\n\u001b[1;32m    680\u001b[0m target_names_pred \u001b[38;5;241m=\u001b[39m {type_name \u001b[38;5;28;01mfor\u001b[39;00m type_name, _, _ \u001b[38;5;129;01min\u001b[39;00m get_entities(y_pred, suffix)}\n",
      "File \u001b[0;32m~/MASTER/ClinicalTrials/venv/lib/python3.10/site-packages/seqeval/metrics/v1.py:390\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, sample_weight, digits, output_dict, zero_division, suffix, scheme)\u001b[0m\n\u001b[1;32m    388\u001b[0m     reporter \u001b[38;5;241m=\u001b[39m DictReporter()\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     name_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     avg_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted avg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    392\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(name_width, avg_width, digits)\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "y_true = [['I-age', 'I-age']]\n",
    "y_pred = [['I-age', 'I-age']]\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode='strict', scheme=IOB2))\n",
    "TP_1_len, FN_1_len, FP_1_len, true_entities_1_len, pred_entities_1_len = compute_cm(y_true, y_pred, suffix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last example, as IOB2 format just consider as entities the ones that start with the prefix B- the `classification_report` function does not recognize any entity. Strict mode does not work well if another scheme (in the example IO) is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both modes compute the metrics in a strict way. On the one hand, default mode considers the entities in IO format, i.e., it does not take into account the prefix for the computation of the metrics. On the other hand, as strict mode works in IOB2 format the entities have to start with a B-entity-name tag and to be predicted correctly all the entity has to match in entity-name and prefix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
