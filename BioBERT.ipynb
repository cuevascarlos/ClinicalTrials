{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioBERT based model\n",
    "\n",
    "#### Written by Carlos Cuevas Villarmín\n",
    "\n",
    "Last update: 29/01/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from csv files\n",
    "\n",
    "path_train = \"./train_full_text.txt\"\n",
    "path_valid = \"./valid_full_text.txt\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(path):\n",
    "    #Read csv file specifying that the first row is the header\n",
    "    df = pd.read_csv(path, sep=\" \", header=0)\n",
    "    return df\n",
    "\n",
    "train = load_data(path_train)\n",
    "valid = load_data(path_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileId</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10459028</td>\n",
       "      <td>A randomized , prospective study of endometria...</td>\n",
       "      <td>O O O O O O B-intervention I-intervention O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11136837</td>\n",
       "      <td>Cardiovascular effects of tamoxifen in women w...</td>\n",
       "      <td>O O O B-intervention O O O O O O O O O O O O O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11283119</td>\n",
       "      <td>Tamoxifen for the prevention of breast cancer ...</td>\n",
       "      <td>B-intervention O O O O O O O O O O O O O O O O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12377957</td>\n",
       "      <td>Tamoxifen , radiation therapy , or both for pr...</td>\n",
       "      <td>B-intervention I-intervention I-intervention I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12393819</td>\n",
       "      <td>Twenty - year follow - up of a randomized stud...</td>\n",
       "      <td>O O O O O O O O O O O B-intervention I-interve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>8523049</td>\n",
       "      <td>Adequate locoregional treatment for early brea...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>9060533</td>\n",
       "      <td>Bisphosphonate risedronate prevents bone loss ...</td>\n",
       "      <td>B-intervention I-intervention O B-condition I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>9672274</td>\n",
       "      <td>Interim analysis of the incidence of breast ca...</td>\n",
       "      <td>O O O O O O O O O O O O O B-intervention O O O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>9678620</td>\n",
       "      <td>Tamoxifen as adjuvant after surgery for breast...</td>\n",
       "      <td>B-intervention O O O O O O O O O O B-control O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>9747868</td>\n",
       "      <td>Tamoxifen for prevention of breast cancer : re...</td>\n",
       "      <td>B-intervention O O O O O O O O O O O O O O O O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>808 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fileId                                           sentence  \\\n",
       "0    10459028  A randomized , prospective study of endometria...   \n",
       "1    11136837  Cardiovascular effects of tamoxifen in women w...   \n",
       "2    11283119  Tamoxifen for the prevention of breast cancer ...   \n",
       "3    12377957  Tamoxifen , radiation therapy , or both for pr...   \n",
       "4    12393819  Twenty - year follow - up of a randomized stud...   \n",
       "..        ...                                                ...   \n",
       "803   8523049  Adequate locoregional treatment for early brea...   \n",
       "804   9060533  Bisphosphonate risedronate prevents bone loss ...   \n",
       "805   9672274  Interim analysis of the incidence of breast ca...   \n",
       "806   9678620  Tamoxifen as adjuvant after surgery for breast...   \n",
       "807   9747868  Tamoxifen for prevention of breast cancer : re...   \n",
       "\n",
       "                                        label_sentence  \n",
       "0    O O O O O O B-intervention I-intervention O O ...  \n",
       "1    O O O B-intervention O O O O O O O O O O O O O...  \n",
       "2    B-intervention O O O O O O O O O O O O O O O O...  \n",
       "3    B-intervention I-intervention I-intervention I...  \n",
       "4    O O O O O O O O O O O B-intervention I-interve...  \n",
       "..                                                 ...  \n",
       "803  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "804  B-intervention I-intervention O B-condition I-...  \n",
       "805  O O O O O O O O O O O O O B-intervention O O O...  \n",
       "806  B-intervention O O O O O O O O O O B-control O...  \n",
       "807  B-intervention O O O O O O O O O O O O O O O O...  \n",
       "\n",
       "[808 rows x 3 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileId             int64\n",
       "sentence          object\n",
       "label_sentence    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Types of the columns\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the elements of each sentence (or text) and save it in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitData(df):\n",
    "    '''\n",
    "    Function that splits the data into words and labels\n",
    "    Args:   \n",
    "        df: pandas dataframe\n",
    "    Returns:\n",
    "        words: list of lists of words\n",
    "        words_labels: list of lists of labels\n",
    "    '''\n",
    "    words   = [sentence.split() for sentence in df['sentence']]\n",
    "    words_labels = [label.split() for label in df['label_sentence']]\n",
    "    print(\"Number of sentences: \", len(words))\n",
    "    print(\"Number of labels: \", len(words_labels))\n",
    "\n",
    "    return words, words_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  808\n",
      "Number of labels:  808\n",
      "Number of sentences:  101\n",
      "Number of labels:  101\n"
     ]
    }
   ],
   "source": [
    "words_train, words_labels_train = SplitData(train)\n",
    "words_valid, words_labels_valid = SplitData(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the words_labels into int values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-total_participants': 0, 'I-cv_bin_percent': 1, 'B-outcome_Measure': 2, 'I-condition': 3, 'B-cv_cont_mean': 4, 'I-location': 5, 'B-iv_cont_q3': 6, 'I-cv_cont_mean': 7, 'B-iv_cont_median': 8, 'B-eligibility': 9, 'I-iv_cont_q3': 10, 'I-age': 11, 'I-outcome': 12, 'B-intervention_participants': 13, 'B-control': 14, 'I-cv_cont_sd': 15, 'I-eligibility': 16, 'I-ethinicity': 17, 'I-outcome_Measure': 18, 'B-ethinicity': 19, 'I-iv_cont_mean': 20, 'B-iv_cont_sd': 21, 'I-iv_cont_sd': 22, 'B-iv_bin_percent': 23, 'O': 24, 'I-cv_cont_median': 25, 'B-condition': 26, 'B-age': 27, 'B-cv_bin_percent': 28, 'I-total_participants': 29, 'B-iv_cont_q1': 30, 'B-outcome': 31, 'B-cv_cont_q3': 32, 'I-cv_cont_q3': 33, 'I-control': 34, 'B-cv_cont_median': 35, 'B-control_participants': 36, 'I-intervention_participants': 37, 'B-cv_cont_q1': 38, 'I-iv_cont_median': 39, 'I-control_participants': 40, 'B-intervention': 41, 'I-intervention': 42, 'I-cv_bin_abs': 43, 'B-iv_cont_mean': 44, 'I-iv_bin_percent': 45, 'I-iv_bin_abs': 46, 'B-iv_bin_abs': 47, 'B-location': 48, 'B-cv_bin_abs': 49, 'B-cv_cont_sd': 50}\n",
      "{0: 'B-total_participants', 1: 'I-cv_bin_percent', 2: 'B-outcome_Measure', 3: 'I-condition', 4: 'B-cv_cont_mean', 5: 'I-location', 6: 'B-iv_cont_q3', 7: 'I-cv_cont_mean', 8: 'B-iv_cont_median', 9: 'B-eligibility', 10: 'I-iv_cont_q3', 11: 'I-age', 12: 'I-outcome', 13: 'B-intervention_participants', 14: 'B-control', 15: 'I-cv_cont_sd', 16: 'I-eligibility', 17: 'I-ethinicity', 18: 'I-outcome_Measure', 19: 'B-ethinicity', 20: 'I-iv_cont_mean', 21: 'B-iv_cont_sd', 22: 'I-iv_cont_sd', 23: 'B-iv_bin_percent', 24: 'O', 25: 'I-cv_cont_median', 26: 'B-condition', 27: 'B-age', 28: 'B-cv_bin_percent', 29: 'I-total_participants', 30: 'B-iv_cont_q1', 31: 'B-outcome', 32: 'B-cv_cont_q3', 33: 'I-cv_cont_q3', 34: 'I-control', 35: 'B-cv_cont_median', 36: 'B-control_participants', 37: 'I-intervention_participants', 38: 'B-cv_cont_q1', 39: 'I-iv_cont_median', 40: 'I-control_participants', 41: 'B-intervention', 42: 'I-intervention', 43: 'I-cv_bin_abs', 44: 'B-iv_cont_mean', 45: 'I-iv_bin_percent', 46: 'I-iv_bin_abs', 47: 'B-iv_bin_abs', 48: 'B-location', 49: 'B-cv_bin_abs', 50: 'B-cv_cont_sd'}\n"
     ]
    }
   ],
   "source": [
    "#Define the tags\n",
    "data = pd.read_csv(\"./dataBIO.txt\", sep=\" \", header=None, names = ['words', 'fileId', 'start', 'end', 'label'])\n",
    "tag_values = list(set(data[\"label\"].values))\n",
    "\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "idx2tag = {i: t for i, t in enumerate(tag_values)}\n",
    "\n",
    "print(tag2idx)\n",
    "print(idx2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map the labels to the tags\n",
    "def MapLabels(words_labels, tag2idx):\n",
    "    '''\n",
    "    Function that maps the labels to the tags\n",
    "    Args:\n",
    "        words_labels: list of lists of labels\n",
    "        tag2idx: dictionary that maps the labels to the tags\n",
    "    Returns:\n",
    "        labels: list of lists of tags\n",
    "    '''\n",
    "    labels = [[tag2idx.get(l) for l in lab] for lab in words_labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = MapLabels(words_labels_train, tag2idx)\n",
    "labels_valid = MapLabels(words_labels_valid, tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to follow the tutorial of HuggingFace [https://huggingface.co/docs/transformers/tasks/token_classification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataset for the train set\n",
    "from datasets import Dataset\n",
    "\n",
    "def CreateDataset(words, words_labels):\n",
    "    '''\n",
    "    Function that creates a dataset with id, words and labels\n",
    "    Args:\n",
    "        words: list of lists of words\n",
    "        words_labels: list of lists of labels\n",
    "    Returns:\n",
    "        dataset: dataset with id, words and labels\n",
    "    '''\n",
    "    dataset = Dataset.from_dict({\"id\": range(len(words)), \"tokens\": words, \"ner_tags\": words_labels})\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'tokens': ['A',\n",
       "  'randomized',\n",
       "  ',',\n",
       "  'prospective',\n",
       "  'study',\n",
       "  'of',\n",
       "  'endometrial',\n",
       "  'resection',\n",
       "  'to',\n",
       "  'prevent',\n",
       "  'recurrent',\n",
       "  'endometrial',\n",
       "  'polyps',\n",
       "  'in',\n",
       "  'women',\n",
       "  'with',\n",
       "  'breast',\n",
       "  'cancer',\n",
       "  'receiving',\n",
       "  'tamoxifen',\n",
       "  '.',\n",
       "  'To',\n",
       "  'assess',\n",
       "  'the',\n",
       "  'role',\n",
       "  'of',\n",
       "  'endometrial',\n",
       "  'resection',\n",
       "  'in',\n",
       "  'preventing',\n",
       "  'recurrence',\n",
       "  'of',\n",
       "  'tamoxifen',\n",
       "  '-',\n",
       "  'associated',\n",
       "  'endometrial',\n",
       "  'polyps',\n",
       "  'in',\n",
       "  'women',\n",
       "  'with',\n",
       "  'breast',\n",
       "  'cancer',\n",
       "  '.',\n",
       "  'Randomized',\n",
       "  ',',\n",
       "  'prospective',\n",
       "  'study',\n",
       "  '(',\n",
       "  'Canadian',\n",
       "  'Task',\n",
       "  'Force',\n",
       "  'classification',\n",
       "  'I',\n",
       "  ')',\n",
       "  '.',\n",
       "  'Tertiary',\n",
       "  'university',\n",
       "  '-',\n",
       "  'affiliated',\n",
       "  'medical',\n",
       "  'center',\n",
       "  '.',\n",
       "  'Twenty',\n",
       "  'consecutive',\n",
       "  'women',\n",
       "  '(',\n",
       "  'age',\n",
       "  'range',\n",
       "  '43',\n",
       "  '-',\n",
       "  '61',\n",
       "  'yrs',\n",
       "  ')',\n",
       "  '.',\n",
       "  'Hysteroscopic',\n",
       "  'removal',\n",
       "  'of',\n",
       "  'tamoxifen',\n",
       "  '-',\n",
       "  'associated',\n",
       "  'endometrial',\n",
       "  'polyps',\n",
       "  'with',\n",
       "  'or',\n",
       "  'without',\n",
       "  'simultaneous',\n",
       "  'resection',\n",
       "  'of',\n",
       "  'the',\n",
       "  'endometrium',\n",
       "  '.',\n",
       "  'Patients',\n",
       "  'were',\n",
       "  'randomized',\n",
       "  'to',\n",
       "  'undergo',\n",
       "  '(',\n",
       "  '10',\n",
       "  'women',\n",
       "  ')',\n",
       "  'or',\n",
       "  'not',\n",
       "  'undergo',\n",
       "  '(',\n",
       "  '10',\n",
       "  ')',\n",
       "  'concomitant',\n",
       "  'endometrial',\n",
       "  'resection',\n",
       "  '.',\n",
       "  'They',\n",
       "  'were',\n",
       "  'followed',\n",
       "  'for',\n",
       "  'at',\n",
       "  'least',\n",
       "  '18',\n",
       "  'months',\n",
       "  '(',\n",
       "  'range',\n",
       "  '18',\n",
       "  '-',\n",
       "  '24',\n",
       "  'mo',\n",
       "  ')',\n",
       "  ',',\n",
       "  'including',\n",
       "  'transvaginal',\n",
       "  'ultrasonography',\n",
       "  'every',\n",
       "  '6',\n",
       "  'months',\n",
       "  'and',\n",
       "  'hysteroscopy',\n",
       "  'when',\n",
       "  'endometrial',\n",
       "  'irregularity',\n",
       "  'was',\n",
       "  'noted',\n",
       "  '.',\n",
       "  'The',\n",
       "  'main',\n",
       "  'outcome',\n",
       "  'variable',\n",
       "  'was',\n",
       "  'recurrence',\n",
       "  'of',\n",
       "  'endometrial',\n",
       "  'polyps',\n",
       "  ';',\n",
       "  'occurrence',\n",
       "  'of',\n",
       "  'uterine',\n",
       "  'bleeding',\n",
       "  'was',\n",
       "  'also',\n",
       "  'noted',\n",
       "  '.',\n",
       "  'In',\n",
       "  'women',\n",
       "  'who',\n",
       "  'underwent',\n",
       "  'endometrial',\n",
       "  'resection',\n",
       "  ',',\n",
       "  'only',\n",
       "  'one',\n",
       "  'had',\n",
       "  'a',\n",
       "  '1',\n",
       "  'x',\n",
       "  '1',\n",
       "  '-',\n",
       "  'cm',\n",
       "  'endometrial',\n",
       "  'polyp',\n",
       "  'diagnosed',\n",
       "  'and',\n",
       "  'removed',\n",
       "  'during',\n",
       "  'follow',\n",
       "  '-',\n",
       "  'up',\n",
       "  '.',\n",
       "  'Seven',\n",
       "  'women',\n",
       "  'remained',\n",
       "  'amenorrheic',\n",
       "  ',',\n",
       "  'and',\n",
       "  'three',\n",
       "  'experienced',\n",
       "  'spotting',\n",
       "  'for',\n",
       "  'a',\n",
       "  'few',\n",
       "  'days',\n",
       "  'every',\n",
       "  'month',\n",
       "  '.',\n",
       "  'In',\n",
       "  'the',\n",
       "  'control',\n",
       "  'group',\n",
       "  ',',\n",
       "  'six',\n",
       "  'women',\n",
       "  'had',\n",
       "  'recurrent',\n",
       "  'endometrial',\n",
       "  'polyps',\n",
       "  'requiring',\n",
       "  'hysteroscopic',\n",
       "  'removal',\n",
       "  '(',\n",
       "  'two',\n",
       "  '-',\n",
       "  'tail',\n",
       "  'Fisher',\n",
       "  \"'s\",\n",
       "  'exact',\n",
       "  'test',\n",
       "  'p',\n",
       "  '<',\n",
       "  '0.06',\n",
       "  ')',\n",
       "  '.',\n",
       "  'Recurrence',\n",
       "  'of',\n",
       "  'endometrial',\n",
       "  'polyps',\n",
       "  ',',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'most',\n",
       "  'common',\n",
       "  'problems',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'breast',\n",
       "  'cancer',\n",
       "  'receiving',\n",
       "  'long',\n",
       "  '-',\n",
       "  'term',\n",
       "  'treatment',\n",
       "  'with',\n",
       "  'tamoxifen',\n",
       "  ',',\n",
       "  'may',\n",
       "  'be',\n",
       "  'reduced',\n",
       "  'by',\n",
       "  'performing',\n",
       "  'endometrial',\n",
       "  'resection',\n",
       "  'at',\n",
       "  'the',\n",
       "  'time',\n",
       "  'of',\n",
       "  'hysteroscopic',\n",
       "  'removal',\n",
       "  'of',\n",
       "  'polyps',\n",
       "  '.',\n",
       "  'The',\n",
       "  'possible',\n",
       "  'risk',\n",
       "  'of',\n",
       "  'occult',\n",
       "  'endometrial',\n",
       "  'cancer',\n",
       "  'is',\n",
       "  'yet',\n",
       "  'to',\n",
       "  'be',\n",
       "  'determined',\n",
       "  '.',\n",
       "  '(',\n",
       "  'J',\n",
       "  'Am',\n",
       "  'Assoc',\n",
       "  'Gynecol',\n",
       "  'Laparosc',\n",
       "  '6(3):285',\n",
       "  '-',\n",
       "  '288',\n",
       "  ',',\n",
       "  '1999',\n",
       "  ')'],\n",
       " 'ner_tags': [24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  41,\n",
       "  42,\n",
       "  24,\n",
       "  24,\n",
       "  26,\n",
       "  3,\n",
       "  3,\n",
       "  24,\n",
       "  9,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  19,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  0,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  27,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  13,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  14,\n",
       "  34,\n",
       "  24,\n",
       "  36,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  2,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  24,\n",
       "  2,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  47,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  31,\n",
       "  12,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  47,\n",
       "  24,\n",
       "  24,\n",
       "  31,\n",
       "  24,\n",
       "  24,\n",
       "  47,\n",
       "  24,\n",
       "  31,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  49,\n",
       "  24,\n",
       "  24,\n",
       "  31,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24]}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = CreateDataset(words_train, labels_train)\n",
    "valid_dataset = CreateDataset(words_valid, labels_valid)\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='dmis-lab/biobert-base-cased-v1.2', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_dataset[0]\n",
    "\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "\n",
    "tokenized_input\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "tokenized_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(data):\n",
    "    tokenized_inputs = tokenizer(data[\"tokens\"], padding='max_length', truncation = True, max_length=42, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(data[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9a3ce34008476d84b0fee52fe9ab39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/808 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a40ca7679e64335a6d3866692e07135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_valid_dataset = valid_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "label_list = list(tag2idx.keys())\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", num_labels=len(tag2idx), id2label=idx2tag, label2id=tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b85c03416d4f5e9ce224e702f302f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b05b939039e4e5b84dd2834941c3efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/MASTER/ClinicalTrials/venv/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Checkpoint destination directory model/checkpoint-51 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28153184056282043, 'eval_precision': 0.5314285714285715, 'eval_recall': 0.5406976744186046, 'eval_f1': 0.5360230547550432, 'eval_accuracy': 0.91524835012157, 'eval_runtime': 11.4244, 'eval_samples_per_second': 8.841, 'eval_steps_per_second': 0.613, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fca96329fe44f8a9e626b07b8ebac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/MASTER/ClinicalTrials/venv/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Checkpoint destination directory model/checkpoint-102 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31829413771629333, 'eval_precision': 0.495, 'eval_recall': 0.5755813953488372, 'eval_f1': 0.5322580645161291, 'eval_accuracy': 0.8996179228898923, 'eval_runtime': 12.5051, 'eval_samples_per_second': 8.077, 'eval_steps_per_second': 0.56, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae2dd39569e48cbad2474af6c284a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/MASTER/ClinicalTrials/venv/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Checkpoint destination directory model/checkpoint-153 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30592969059944153, 'eval_precision': 0.518324607329843, 'eval_recall': 0.5755813953488372, 'eval_f1': 0.5454545454545455, 'eval_accuracy': 0.9058700937825633, 'eval_runtime': 11.5804, 'eval_samples_per_second': 8.722, 'eval_steps_per_second': 0.604, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1068.7449, 'train_samples_per_second': 2.268, 'train_steps_per_second': 0.143, 'train_loss': 0.16100337458591835, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=153, training_loss=0.16100337458591835, metrics={'train_runtime': 1068.7449, 'train_samples_per_second': 2.268, 'train_steps_per_second': 0.143, 'train_loss': 0.16100337458591835, 'epoch': 3.0})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "BB_training_args = TrainingArguments(\n",
    "    output_dir=\"model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    adam_epsilon=1e-8,\n",
    "    adam_beta1=0.9,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=BB_training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
