{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioBERT based model\n",
    "\n",
    "#### Written by Carlos Cuevas Villarm√≠n\n",
    "\n",
    "Last update: 29/01/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Load the Dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cuevascarlos/PICO-breast-cancer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 808\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 101\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 102\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'tokens': ['A',\n",
       "  'multicenter',\n",
       "  'randomized',\n",
       "  'trial',\n",
       "  'of',\n",
       "  'the',\n",
       "  'effects',\n",
       "  'of',\n",
       "  'exercise',\n",
       "  'dose',\n",
       "  'and',\n",
       "  'type',\n",
       "  'on',\n",
       "  'psychosocial',\n",
       "  'distress',\n",
       "  'in',\n",
       "  'breast',\n",
       "  'cancer',\n",
       "  'patients',\n",
       "  'undergoing',\n",
       "  'chemotherapy',\n",
       "  '.',\n",
       "  'Exercise',\n",
       "  'may',\n",
       "  'improve',\n",
       "  'psychosocial',\n",
       "  'distress',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'cancer',\n",
       "  ';',\n",
       "  'however',\n",
       "  ',',\n",
       "  'few',\n",
       "  'studies',\n",
       "  'have',\n",
       "  'examined',\n",
       "  'the',\n",
       "  'effects',\n",
       "  'of',\n",
       "  'different',\n",
       "  'types',\n",
       "  'or',\n",
       "  'doses',\n",
       "  'of',\n",
       "  'exercise',\n",
       "  ',',\n",
       "  'or',\n",
       "  'whether',\n",
       "  'exercise',\n",
       "  'effects',\n",
       "  'are',\n",
       "  'related',\n",
       "  'to',\n",
       "  'baseline',\n",
       "  'depression',\n",
       "  'levels',\n",
       "  '.',\n",
       "  'In',\n",
       "  'a',\n",
       "  'multicenter',\n",
       "  'trial',\n",
       "  'in',\n",
       "  'Canada',\n",
       "  ',',\n",
       "  'we',\n",
       "  'randomized',\n",
       "  '301',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'breast',\n",
       "  'cancer',\n",
       "  'initiating',\n",
       "  'chemotherapy',\n",
       "  'to',\n",
       "  'thrice',\n",
       "  'weekly',\n",
       "  ',',\n",
       "  'supervised',\n",
       "  'exercise',\n",
       "  'consisting',\n",
       "  'of',\n",
       "  'either',\n",
       "  'a',\n",
       "  'standard',\n",
       "  'dose',\n",
       "  'of',\n",
       "  '25',\n",
       "  'to',\n",
       "  '30',\n",
       "  'minutes',\n",
       "  'of',\n",
       "  'aerobic',\n",
       "  'exercise',\n",
       "  '(',\n",
       "  'STAN',\n",
       "  ';',\n",
       "  'n',\n",
       "  '=',\n",
       "  '96',\n",
       "  ')',\n",
       "  ',',\n",
       "  'a',\n",
       "  'higher',\n",
       "  'dose',\n",
       "  'of',\n",
       "  '50',\n",
       "  'to',\n",
       "  '60',\n",
       "  'minutes',\n",
       "  'of',\n",
       "  'aerobic',\n",
       "  'exercise',\n",
       "  '(',\n",
       "  'HIGH',\n",
       "  ';',\n",
       "  'n',\n",
       "  '=',\n",
       "  '101',\n",
       "  ')',\n",
       "  ',',\n",
       "  'or',\n",
       "  'a',\n",
       "  'combined',\n",
       "  'dose',\n",
       "  'of',\n",
       "  '50',\n",
       "  'to',\n",
       "  '60',\n",
       "  'minutes',\n",
       "  'of',\n",
       "  'aerobic',\n",
       "  'and',\n",
       "  'resistance',\n",
       "  'exercise',\n",
       "  '(',\n",
       "  'COMB',\n",
       "  ';',\n",
       "  'n',\n",
       "  '=',\n",
       "  '104',\n",
       "  ')',\n",
       "  '.',\n",
       "  'The',\n",
       "  'primary',\n",
       "  'endpoint',\n",
       "  'was',\n",
       "  'depression',\n",
       "  'assessed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'Center',\n",
       "  'for',\n",
       "  'Epidemiological',\n",
       "  'Studies',\n",
       "  '-',\n",
       "  'Depression',\n",
       "  'scale',\n",
       "  'at',\n",
       "  'baseline',\n",
       "  ',',\n",
       "  'twice',\n",
       "  'during',\n",
       "  'chemotherapy',\n",
       "  ',',\n",
       "  'and',\n",
       "  'postchemotherapy',\n",
       "  '.',\n",
       "  'Secondary',\n",
       "  'endpoints',\n",
       "  'were',\n",
       "  'anxiety',\n",
       "  ',',\n",
       "  'perceived',\n",
       "  'stress',\n",
       "  ',',\n",
       "  'and',\n",
       "  'self',\n",
       "  '-',\n",
       "  'esteem',\n",
       "  '.',\n",
       "  'Repeated',\n",
       "  'measures',\n",
       "  'ANOVA',\n",
       "  'indicated',\n",
       "  'that',\n",
       "  'neither',\n",
       "  'HIGH',\n",
       "  '[',\n",
       "  'mean',\n",
       "  'difference',\n",
       "  '=',\n",
       "  '-',\n",
       "  '0',\n",
       "  '.',\n",
       "  '9',\n",
       "  ';',\n",
       "  '95',\n",
       "  '%',\n",
       "  'confidence',\n",
       "  'interval',\n",
       "  '(',\n",
       "  'CI',\n",
       "  ')',\n",
       "  ',',\n",
       "  '+',\n",
       "  '0',\n",
       "  '.',\n",
       "  '0',\n",
       "  'to',\n",
       "  '-',\n",
       "  '1',\n",
       "  '.',\n",
       "  '8',\n",
       "  ';',\n",
       "  'P',\n",
       "  '=',\n",
       "  '0',\n",
       "  '.',\n",
       "  '061',\n",
       "  ']',\n",
       "  'nor',\n",
       "  'COMB',\n",
       "  '(',\n",
       "  'mean',\n",
       "  'difference',\n",
       "  '=',\n",
       "  '-',\n",
       "  '0',\n",
       "  '.',\n",
       "  '4',\n",
       "  ';',\n",
       "  '95',\n",
       "  '%',\n",
       "  'CI',\n",
       "  ',',\n",
       "  '+',\n",
       "  '0',\n",
       "  '.',\n",
       "  '5',\n",
       "  'to',\n",
       "  '-',\n",
       "  '1',\n",
       "  '.',\n",
       "  '3',\n",
       "  ';',\n",
       "  'P',\n",
       "  '=',\n",
       "  '0',\n",
       "  '.',\n",
       "  '36',\n",
       "  ')',\n",
       "  'was',\n",
       "  'superior',\n",
       "  'to',\n",
       "  'STAN',\n",
       "  'for',\n",
       "  'managing',\n",
       "  'depressive',\n",
       "  'symptoms',\n",
       "  '.',\n",
       "  'In',\n",
       "  'a',\n",
       "  'planned',\n",
       "  'subgroup',\n",
       "  'analysis',\n",
       "  ',',\n",
       "  'there',\n",
       "  'was',\n",
       "  'a',\n",
       "  'significant',\n",
       "  'interaction',\n",
       "  'with',\n",
       "  'baseline',\n",
       "  'depression',\n",
       "  'levels',\n",
       "  '(',\n",
       "  'P',\n",
       "  'interaction',\n",
       "  '=',\n",
       "  '0',\n",
       "  '.',\n",
       "  '027',\n",
       "  ')',\n",
       "  'indicating',\n",
       "  'that',\n",
       "  'COMB',\n",
       "  'and',\n",
       "  'HIGH',\n",
       "  'were',\n",
       "  'effective',\n",
       "  'for',\n",
       "  'managing',\n",
       "  'depressive',\n",
       "  'symptoms',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'clinical',\n",
       "  'levels',\n",
       "  'of',\n",
       "  'depressive',\n",
       "  'symptoms',\n",
       "  'at',\n",
       "  'baseline',\n",
       "  '.',\n",
       "  'Compared',\n",
       "  'with',\n",
       "  'a',\n",
       "  'standard',\n",
       "  'volume',\n",
       "  'of',\n",
       "  'aerobic',\n",
       "  'exercise',\n",
       "  ',',\n",
       "  'higher',\n",
       "  'volumes',\n",
       "  'of',\n",
       "  'exercise',\n",
       "  'did',\n",
       "  'not',\n",
       "  'help',\n",
       "  'manage',\n",
       "  'depressive',\n",
       "  'symptoms',\n",
       "  'in',\n",
       "  'unselected',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'breast',\n",
       "  'cancer',\n",
       "  'receiving',\n",
       "  'chemotherapy',\n",
       "  ',',\n",
       "  'but',\n",
       "  'they',\n",
       "  'were',\n",
       "  'effective',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'clinical',\n",
       "  'levels',\n",
       "  'of',\n",
       "  'depressive',\n",
       "  'symptoms',\n",
       "  'at',\n",
       "  'baseline',\n",
       "  '.',\n",
       "  'A',\n",
       "  'phase',\n",
       "  'III',\n",
       "  'exercise',\n",
       "  'trial',\n",
       "  'targeting',\n",
       "  'depressed',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'breast',\n",
       "  'cancer',\n",
       "  'is',\n",
       "  'warranted',\n",
       "  '.'],\n",
       " 'ner_tags': [15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  26,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  28,\n",
       "  11,\n",
       "  15,\n",
       "  3,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  18,\n",
       "  3,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  32,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  32,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  32,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  23,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  23,\n",
       "  15,\n",
       "  23,\n",
       "  35,\n",
       "  15,\n",
       "  15,\n",
       "  23,\n",
       "  35,\n",
       "  35,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  49,\n",
       "  22,\n",
       "  22,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  49,\n",
       "  22,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  49,\n",
       "  22,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same dictionaries when Dataset was created\n",
    "tag2idx  = {'I-iv-bin-percent': 0, 'I-control-participants': 1, 'B-cv-bin-percent': 2, 'B-eligibility': 3, 'B-age': 4, 'I-age': 5, 'I-intervention-participants': 6, 'B-cv-cont-q1': 7, 'I-cv-cont-q3': 8, 'B-iv-cont-mean': 9, 'B-location': 10, 'I-condition': 11, 'I-cv-bin-percent': 12, 'B-iv-cont-q1': 13, 'I-control': 14, 'O': 15, 'B-cv-cont-sd': 16, 'I-intervention': 17, 'B-total-participants': 18, 'I-iv-cont-mean': 19, 'B-iv-cont-sd': 20, 'I-cv-bin-abs': 21, 'I-outcome': 22, 'B-outcome-Measure': 23, 'I-iv-cont-q3': 24, 'B-cv-cont-median': 25, 'B-intervention': 26, 'B-cv-bin-abs': 27, 'B-condition': 28, 'I-cv-cont-sd': 29, 'B-control-participants': 30, 'B-iv-cont-median': 31, 'B-intervention-participants': 32, 'I-iv-cont-median': 33, 'B-iv-cont-q3': 34, 'I-outcome-Measure': 35, 'B-cv-cont-mean': 36, 'I-iv-bin-abs': 37, 'I-iv-cont-sd': 38, 'B-control': 39, 'B-ethinicity': 40, 'I-cv-cont-median': 41, 'I-location': 42, 'Entity': 43, 'I-ethinicity': 44, 'I-cv-cont-mean': 45, 'B-iv-bin-percent': 46, 'B-iv-bin-abs': 47, 'I-total-participants': 48, 'B-outcome': 49, 'I-eligibility': 50, 'B-cv-cont-q3': 51}\n",
    "idx2tag = {0: 'I-iv-bin-percent', 1: 'I-control-participants', 2: 'B-cv-bin-percent', 3: 'B-eligibility', 4: 'B-age', 5: 'I-age', 6: 'I-intervention-participants', 7: 'B-cv-cont-q1', 8: 'I-cv-cont-q3', 9: 'B-iv-cont-mean', 10: 'B-location', 11: 'I-condition', 12: 'I-cv-bin-percent', 13: 'B-iv-cont-q1', 14: 'I-control', 15: 'O', 16: 'B-cv-cont-sd', 17: 'I-intervention', 18: 'B-total-participants', 19: 'I-iv-cont-mean', 20: 'B-iv-cont-sd', 21: 'I-cv-bin-abs', 22: 'I-outcome', 23: 'B-outcome-Measure', 24: 'I-iv-cont-q3', 25: 'B-cv-cont-median', 26: 'B-intervention', 27: 'B-cv-bin-abs', 28: 'B-condition', 29: 'I-cv-cont-sd', 30: 'B-control-participants', 31: 'B-iv-cont-median', 32: 'B-intervention-participants', 33: 'I-iv-cont-median', 34: 'B-iv-cont-q3', 35: 'I-outcome-Measure', 36: 'B-cv-cont-mean', 37: 'I-iv-bin-abs', 38: 'I-iv-cont-sd', 39: 'B-control', 40: 'B-ethinicity', 41: 'I-cv-cont-median', 42: 'I-location', 43: 'Entity', 44: 'I-ethinicity', 45: 'I-cv-cont-mean', 46: 'B-iv-bin-percent', 47: 'B-iv-bin-abs', 48: 'I-total-participants', 49: 'B-outcome', 50: 'I-eligibility', 51: 'B-cv-cont-q3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='dmis-lab/biobert-base-cased-v1.2', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', 'multi', '##cent', '##er', 'random', '##ized', 'trial', 'of', 'the', 'effects', 'of', 'exercise', 'dose', 'and', 'type', 'on', 'ps', '##ych', '##oso', '##cial', 'distress', 'in', 'breast', 'cancer', 'patients', 'undergoing', 'ch', '##em', '##otherapy', '.', 'exercise', 'may', 'improve', 'ps', '##ych', '##oso', '##cial', 'distress', 'in', 'patients', 'with', 'cancer', ';', 'however', ',', 'few', 'studies', 'have', 'examined', 'the', 'effects', 'of', 'different', 'types', 'or', 'doses', 'of', 'exercise', ',', 'or', 'whether', 'exercise', 'effects', 'are', 'related', 'to', 'base', '##line', 'depression', 'levels', '.', 'in', 'a', 'multi', '##cent', '##er', 'trial', 'in', 'can', '##ada', ',', 'we', 'random', '##ized', '301', 'patients', 'with', 'breast', 'cancer', 'in', '##iti', '##ating', 'ch', '##em', '##otherapy', 'to', 'th', '##rice', 'weekly', ',', 'supervised', 'exercise', 'consisting', 'of', 'either', 'a', 'standard', 'dose', 'of', '25', 'to', '30', 'minutes', 'of', 'a', '##ero', '##bic', 'exercise', '(', 's', '##tan', ';', 'n', '=', '96', ')', ',', 'a', 'higher', 'dose', 'of', '50', 'to', '60', 'minutes', 'of', 'a', '##ero', '##bic', 'exercise', '(', 'high', ';', 'n', '=', '101', ')', ',', 'or', 'a', 'combined', 'dose', 'of', '50', 'to', '60', 'minutes', 'of', 'a', '##ero', '##bic', 'and', 'resistance', 'exercise', '(', 'comb', ';', 'n', '=', '104', ')', '.', 'the', 'primary', 'end', '##point', 'was', 'depression', 'assessed', 'by', 'the', 'center', 'for', 'e', '##pid', '##em', '##iol', '##ogical', 'studies', '-', 'depression', 'scale', 'at', 'base', '##line', ',', 'twice', 'during', 'ch', '##em', '##otherapy', ',', 'and', 'post', '##che', '##mother', '##ap', '##y', '.', 'secondary', 'end', '##points', 'were', 'anxiety', ',', 'perceived', 'stress', ',', 'and', 'self', '-', 'esteem', '.', 'repeated', 'measures', 'an', '##ova', 'indicated', 'that', 'neither', 'high', '[', 'mean', 'difference', '=', '-', '0', '.', '9', ';', '95', '%', 'confidence', 'interval', '(', 'c', '##i', ')', ',', '+', '0', '.', '0', 'to', '-', '1', '.', '8', ';', 'p', '=', '0', '.', '06', '##1', ']', 'nor', 'comb', '(', 'mean', 'difference', '=', '-', '0', '.', '4', ';', '95', '%', 'c', '##i', ',', '+', '0', '.', '5', 'to', '-', '1', '.', '3', ';', 'p', '=', '0', '.', '36', ')', 'was', 'superior', 'to', 's', '##tan', 'for', 'managing', 'de', '##pressive', 'symptoms', '.', 'in', 'a', 'planned', 'subgroup', 'analysis', ',', 'there', 'was', 'a', 'significant', 'interaction', 'with', 'base', '##line', 'depression', 'levels', '(', 'p', 'interaction', '=', '0', '.', '02', '##7', ')', 'indicating', 'that', 'comb', 'and', 'high', 'were', 'effective', 'for', 'managing', 'de', '##pressive', 'symptoms', 'in', 'patients', 'with', 'clinical', 'levels', 'of', 'de', '##pressive', 'symptoms', 'at', 'base', '##line', '.', 'compared', 'with', 'a', 'standard', 'volume', 'of', 'a', '##ero', '##bic', 'exercise', ',', 'higher', 'volumes', 'of', 'exercise', 'did', 'not', 'help', 'manage', 'de', '##pressive', 'symptoms', 'in', 'un', '##sel', '##ec', '##ted', 'patients', 'with', 'breast', 'cancer', 'receiving', 'ch', '##em', '##otherapy', ',', 'but', 'they', 'were', 'effective', 'in', 'patients', 'with', 'clinical', 'levels', 'of', 'de', '##pressive', 'symptoms', 'at', 'base', '##line', '.', 'a', 'phase', 'ii', '##i', 'exercise', 'trial', 'targeting', 'depressed', 'patients', 'with', 'breast', 'cancer', 'is', 'warrant', '##ed', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "\n",
    "tokenized_input\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(data):\n",
    "    tokenized_inputs = tokenizer(data[\"tokens\"], padding='max_length', truncation = True, max_length=512, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(data[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba98957f42584c118ada115bc3546ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = dataset['train'].map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_valid_dataset = dataset['valid'].map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "label_list = list(tag2idx.keys())\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", num_labels=len(tag2idx), id2label=idx2tag, label2id=tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228641fdfa184cff965f0ce4c72fd269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    adam_epsilon=1e-8,\n",
    "    adam_beta1=0.9,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
